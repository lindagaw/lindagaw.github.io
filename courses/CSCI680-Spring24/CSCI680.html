
<!DOCTYPE html>
<html>

<head>
<meta charset="utf-8">
<title>CSCI 680</title>
<link rel='stylesheet' type='text/css' href='style.css'>
</head>

<body>
<h1>CSCI 680 Spring 2024: Deep Transfer Learning</h1>

<img src="https://hub.packtpub.com/wp-content/uploads/2018/03/501_Cover-Image_0.png" width="100%"  style="padding-top:14px;padding-bottom:14px">


<h2 id="toc_2">Overview</h2>

<p>Deep Transfer Learning is an advanced graduate-level course that explores the intersection of deep learning and transfer learning techniques in the field of machine learning. Specifically, this course focuses on the two major advances in the field of deep transfer learning in recent years: unsupervised domain adaptation and domain generalization. This course equips students with the knowledge and practical skills necessary to develop deep learning algorithms generalizable to new data. It delves into the practical methodologies and cutting-edge research developments in the domain of deep transfer learning.</p>

</body>

</html>

<h2 id="toc_8">Useful Links</h2>
<ul>

  <li>
    The piazza signup link is <a href="https://piazza.com/wm/spring2024/csci680">here</a>.
  </li>

  <li>
    The <a href="https://lindagaw.github.io/courses/CSCI680/CSCI680.html">course website</a> (this website) is available.
  </li>

  <li>
    The instructor's <a href="https://lindagaw.github.io/">homepage</a> is available.
  </li>

</ul>

<h2 id="toc_11">Important Dates</h2>
<table>
  <tr>
    <td><b>Agenda Item</b></td>
    <td><b>Due Date</b></td>
    <td><b>Time</b></td>
    <td><b>Location</b></td>

  </tr>

  <tr>
    <td>Final Project Report</td>
    <td>05/03</td>
    <td>23:59</td>
    <td>Blackboard</td>
  </tr>

  <tr>
    <td>Final Exam</td>
    <td>TBA</td>
    <td>TBA</td>
    <td>McGlothlin-Street Hall Room 2 (tentative)</td>
  </tr>

</table>

<h2 id="toc_3">Syllabus</h2>
<p>The <a href="./Documents/Syllabus_CSCI_680.pdf">syllabus</a> is available.




<h2 id="toc_4">Where and When</h2>

<p>Class will be held synchronously every week, including a combination of lectures and office hours. Students are encouraged to attend both the lectures and office hours each week. There will be two mandatory tests held. Students are also encouraged to reach out to the instructor(s) with any questions or concerns.</p>


<table>
  <tr>
    <td><b>Instructor</b></td>
    <td><b>Lecture Time</b></td>
    <td><b>Lecture Location</b></td>
    <td><b>Office Hours</b></td>
    <td><b>Office Location</b></td>
    <td><b>Email</b></td>
  </tr>


  <tr>
    <td>Ashley Gao</td>
    <td>T/R: 12:30-13:50</td>
    <td>McGlothlin-Street Hall Room 2</td>
    <td>W/F: 10:00-11:30</td>
    <td>McGlothlin-Street Hall 004</td>
    <td>ygao18@wm.edu</td>
  </tr>


</table>



<h2 id="toc_5">Homework(s)</h2>

<p>This class will have 1 homework. Please come back to this page once it is annouced during the class that a homework is posted. The homeworks are collected using Blackboard.</p>

<p>The homework is a <b>programming assignment</b>. They will test your knowledge on the basics of deep learning as well as deep transfer learning (unsupervised domain adaptation and domain generalization). They will not be about reviews about your paper reading.</p>

<table>
  <tr>
    <td><b>&#35;</b></td>
    <td><b>Out</b></td>
    <td><b>Due</b></td>
    <td><b>Materials</b></td>

  </tr>
  <tr>
    <td><b>1</b></td>
    <td>Feb 20</td>
    <td>Apr 20</td>
    <td><a href="Homeworks/CSCI_680_Homework.pdf">[Homework]</a> </td>
  </tr>

</table>



<h2 id="toc_3">Grading</h2>
<ul>
  <li>
    Homework: 25 pts
  </li>
  <li>
    Paper Presentation: 15 pts
  </li>
  <li>
    Final Exam: 25 pts
  </li>
  <li>
    Final Project: 25 pts
  </li>
  <li>
    Attendance: 10 pts
  </li>

  <p>Final letter grades will be given based on the scale detailed in the syllabus. More specifically, A >= 93% > A- >= 90% > B+ >= 87% > B >= 83% > B- >= 80% > C+ >= 77% > C >= 73% > C- >= 70% > D+ >= 67% > D >= 65% > D- >= 60% > F. Your grades may be curved at the instructor's discretion.
</ul>

<h2 id="toc_10">Lecture Schedule</h2>
<p>Note that this schedule is tentative and will be updated once a topic is covered in the lecture(s). </p>

<table width=1000>
  <tr>
    <td width="4%"><b>&#35;</b></td>
    <td width="8%"><b>Dates</b></td>
    <td width="25%"><b>Topic</b> </td>
    <td width="25%"><b>Materials</b></td>
    <td width="40%"><b>Instructor Notes</b></td>
  </tr>

  <tr>
    <td><b>0</b></td>
    <td>01/25</td>
    <td>
      <b>Lecture</b>: Class Cancelled

    </td>
    <td>
      <b>Lecture:</b> No Class <br/>
    </td>

    <td>
      The syllabus is available at <a href="./Documents/Syllabus_CSCI_680.pdf">here</a>.

    </td>

  </tr>

  <tr>
    <td><b>1</b></td>
    <td>01/30</td>
    <td>
      <b>Lecture</b>: Logistics & Introduction to Deep Transfer Learning

    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_1_Intro_DTL.pdf">[Lecture]</a> <br/>
    </td>

    <td>
      Zhu, et al (2023): <a href="https://www.youtube.com/watch?v=kYRkDd3C4gs&ab_channel=XiangYu"> Visual Domain Adaptation and Generalization </a>

    </td>

  </tr>


  <tr>
    <td><b>2</b></td>
    <td>02/01, 02/06</td>
    <td>
      <b>Lecture</b>: Linear Models & Optimization

    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_2_Linear_Regression_Optimization.pdf">[Lecture]</a> <br/>
      <b>Tutorial:</b> <a href="Tutorials/Tutorial_2_Linear_Regression.ipynb">[Tutorial]</a> <br/>

    </td>

    <td>
      Ng (2018): <a href="https://www.youtube.com/watch?v=4b4MUYve_U8&ab_channel=StanfordOnline">Linear Regression & Gradient Descent</a>
    </td>

  </tr>

  <tr>
    <td><b>3</b></td>
    <td>02/08, 02/13, 02/15</td>
    <td>
      <b>Lecture</b>: Logistic Regression, Multiclass Classification

    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_3_Logistic_Regression.pdf">[Lecture]</a> <br/>
      <b>Tutorial:</b> <a href="Tutorials/Tutorial_3_Logistic_Regression.ipynb">[Tutorial]</a> <br/>

    </td>

    <td>
      Ng (2017): <a href="https://www.youtube.com/watch?v=4u81xU7BIOc&ab_channel=MachineLearningandAI">Logistic Regression</a>
    </td>

  </tr>

  <tr>
    <td><b>4</b></td>
    <td>02/20, 02/27</td>
    <td>
      <b>Lecture:</b> Multilayer Perceptrons<br/>
      <b>Tutorial:</b> MLP <br/>
    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_4_Multilayer_Perceptrons.pdf">[Lecture]</a> <br/>
      <b>Tutorial:</b> <a href="Tutorials/Tutorial_4_MLP.ipynb">[Tutorial]</a> </br>

    </td>

    <td>
      Stanford: <a href="http://deeplearning.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">Multilayer Perceptrons</a>
    </td>

  </tr>

  <tr>
    <td><b>5</b></td>
    <td>02/29, 03/05</td>
    <td>
      <b>Lecture:</b> Convolutional Neural Networks<br/>
      <b>Tutorial:</b> CNN <br/>
    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_5_CNN.pdf">[Lecture]</a> <br/>
      <b>Tutorial:</b> <a href="Tutorials/Tutorial_5_CNN.ipynb">[Tutorial]</a> </br>

    </td>

    <td>
      Ng: <a href="https://www.youtube.com/watch?v=ArPaAX_PhIs&list=PLcCe-ymWq77rTVo-pN2X7qsfBTnmrsYS7&ab_channel=DeepLearningAI">CNN for Computer Vision</a><br/>
    </td>

  </tr>

  <tr>
    <td><b>6</b></td>
    <td>03/07, 03/19</td>
    <td>
      <b>Lecture:</b> Recurrent Neural Networks & Variations <br/>
    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_6_RNN.pdf">[Lecture]</a> <br/>

    </td>

    <td>
      Stanford: <a href="https://www.youtube.com/watch?v=6niqTuYFZLQ&ab_channel=StanfordUniversitySchoolofEngineering">Recurrent Neural Networks</a><br/>
    </td>

  </tr>

  <tr>
    <td><b>7</b></td>
    <td>03/21, 03/26</td>
    <td>
      <b>Lecture:</b> Graph Neural Networks<br/>
    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_7_GNN.pdf">[Lecture]</a> <br/>

    </td>

    <td>
      Microsoft: <a href="https://www.youtube.com/watch?v=zCEYiCxrL_0&ab_channel=MicrosoftResearch">An Introduction to Graph Neural Networks</a><br/>
    </td>

  </tr>

  <tr>
    <td><b>8</b></td>
    <td>03/28, 04/02</td>
    <td>
      <b>Lecture:</b> Attention & Transformers<br/>
      <b>Tutorial:</b> Vision Transformers <br/>
    </td>
    <td>
      <b>Lecture:</b> <a href="Lectures/Lecture_8_Attention.pdf">[Lecture]</a> <br/>
      <b>Tutorial:</b> <a href="Tutorials/Tutorial_8_Vision_Transformers.py">[Tutorial]</a> </br>

    </td>
    <td>
      Phi: <a href="https://www.youtube.com/watch?v=4Bdc55j80l8&ab_channel=TheA.I.Hacker-MichaelPhi">Illustrated Guide to Transformers</a><br/>
      Vaswani: <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a><br/>
    </td>
  </tr>


</table>



<h2 id="toc_10">Presentation Schedule</h2>

<p>Please signup to present one paper per student. Sign up link is <a href="https://docs.google.com/document/d/1i2GRRxOkVqiEgDQ17WCzGz1_mTV2TzBWIF0GhdSQHs8/edit?usp=sharing">here</a>. Before you present a paper, please submit your slides to Blackboard. Since there are 3 presentations per class, please time your presentation wisely. If you go significantly over time, you will be cut off.</p>

<table width=1000>
  <tr>
    <td width="4%"><b>&#35;</b></td>
    <td width="8%"><b>Dates</b></td>
    <td width="70%"><b>Topic</b> </td>
    <td width="18%"><b>Instructor Notes</b></td>
  </tr>

  <tr>
    <td><b>1</b></td>
    <td>04/04</td>
    <td>
      <b>Paper</b>: Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation <br/>
      <b>Paper</b>: A Closer Look at Smoothness in Domain Adversarial Training <br/>
      <b>Paper</b>: Adversarial Unsupervised Domain Adaptation With Conditional and Label Shift: Infer, Align and Iterate <br/>
    </td>

    <td>
      Student presentations. <br/>
    </td>
  </tr>

  <tr>
    <td><b>2</b></td>
    <td>04/09</td>
    <td>
      <b>Paper</b>: Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation <br/>
      <b>Paper</b>: Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation <br/>
      <b>Paper</b>: Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation <br/>
    </td>
    <td>
      Student presentations. <br/>
    </td>
  </tr>

  <tr>
    <td><b>3</b></td>
    <td>04/11</td>
    <td>
      <b>Paper</b>: MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation <br/>
      <b>Paper</b>: Self-adaptive Re-weighted Adversarial Domain Adaptation <br/>
      <b>Paper</b>: Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation <br/>
    </td>
    <td>
      Student presentations. <br/>
    </td>
  </tr>

  <tr>
    <td><b>4</b></td>
    <td>04/16</td>
    <td>
      <b>Paper</b>: Domain Conditioned Adaptation Network <br/>
      <b>Paper</b>: HoMM: Higher-order Moment Matching for Unsupervised Domain Adaptation <br/>
      <b>Paper</b>: Unsupervised Domain Adaptation with Residual Transfer Networks <br/>
    </td>
    <td>
      Student presentations. <br/>
    </td>
  </tr>

  <tr>
    <td><b>5</b></td>
    <td>04/18</td>
    <td>
      <b>Paper</b>: Domain Generalization: A Survey <br/>
      <b>Paper</b>: Conditional Prompt Learning for Vision-Language Models <br/>
      <b>Paper</b>: Domain generalization via conditional invariant representations <br/>
    </td>
    <td>
      Student presentations. <br/>
      For the first paper, please present the one written by K. Zhou et al. <br/>
    </td>
  </tr>

  <tr>
    <td><b>6</b></td>
    <td>04/23</td>
    <td>
      <b>Paper</b>: Unified deep supervised domain adaptation and generalizatio <br/>
      <b>Paper</b>: Metanorm: Learning to normalize few-shot batches across domains <br/>
      <b>Paper</b>: Meta-Learning for Domain Generalization in Semantic Parsing <br/>
    </td>
    <td>
      Student presentations. <br/>
    </td>
  </tr>


</table>


<h2 id="toc_19">Final Project</h2>

<p>25% of your total mark is allocated to a final project, which will require you to apply several algorithms to a challenge problem and to write a short report analyzing the results. The final project is an individual project, meaning that you are not allowed to collaborate with others. </p>
