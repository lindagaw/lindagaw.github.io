
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Ashley (Ye) Gao</title>
   <meta name="author" content="" />

   <!--- Blueprint CSS Framework -->
   <link rel="stylesheet" href="css/blueprint/screen.css" type="text/css" media="screen, projection" />
   <link rel="stylesheet" href="css/blueprint/print.css" type="text/css" media="print" />
   <!--[if IE]>
      <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
   <![endif]-->

   <!-- CodeRay syntax highlighting CSS -->
   <link rel="stylesheet" href="css/coderay.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="css/site.css" type="text/css" media="screen, projection" />
</head>
<body>

<div class="container">




   <div class="column span-15 prepend-2 append-2 first last" id="header">
     <p class="title">Ashley (Ye) Gao</p>

     <div id="navigation">
		<ul>


				<li><a href="http://lindagaw.github.io">Home</a></li>


				<li><a href="./group.html">Group</a></li>


			   <li class="current"> Publications </li>


				<li><a href="./teaching.html">Teaching</a></li>

        <li><a href="./awards.html">Awards</a></li>

		</ul>
	</div>

     <hr />
   </div>



   <div class="column span-15 prepend-2 first">
      <h3>Publications</h3>

      <h5>2023</h5>
      <ul>
          <li> [ArXiv] Gao, Y., Jabbour, J., Ko, E., Wijayasingha, L., Kim, S., Wang, Z., Ma, M., Rose, K., Gordon, K., Wang, H., Stankovic, J. A., “Integrating Voice-Based Machine Learning Technology into Complex Home Environments”, arXiv preprint arXiv:2211.03149. </li>
          <li> [ArXiv] Gao, Y., Chu, Z., Wang, H., Stankovic, J. “MiddleGAN: Generate Domain Agnostic Samples for Unsupervised Domain Adaptation”, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases. (submitted) </li>
          <li> [SmartComp'23] Gao, Y., Baucom, B., Gordon, K., Rose, K., Wang, H., Stankovic. “E-ADDA: Unsupervised Adversarial Domain Adaptation Enhanced by a New Mahalanobis Distance Loss for Smart Computing”, arXiv preprint arXiv:2201.10001. (accepted). </li>
      </ul>

      <h5>2022</h5>
      <ul>
        <li> [INA'22] Rose, K., Gordon, K., Schlegel, E., McCall, M., Gao, Y., Jabbour, J. and Ko, E., 2021. “Pandemic Deployment of a Smarthealth Technology to Improve Stress in Dementia Family Caregivers”, Innovation in Aging, 5(Suppl 1), pp.450-450. </li>
      </ul>

      <h5>2021</h5>
      <ul>
        <li> [HEALTH'21] Gao, Y, Salekin, A., Gordon, K., Rose, K., Wang, H. and Stankovic, J., 2021. “Emotion Recognition Robust to Indoor Environmental Distortions and Non-targeted Emotions Using Out-of-distribution Detection”, ACM Transactions on Computing for Healthcare (HEALTH), 3(2), pp.1-22.</li>
        <li> [PERCOM’21] Gao, Y., Jabbour, J., Schlegel, E.C., Ma, M., McCall, M., Wijayasingha, L., Ko, E., Gordon, K., Rose, K., Wang, H. and Stankovic, J., 2021. “Out-of-the-Box Deployment to Support Research on In-Home Care of Alzheimer’s Patients”, IEEE Pervasive Computing, 21(1), pp.37-47. </li>
        <li> [JAN’21] Rose, K.M., Coop Gordon, K., Schlegel, E.C., Mccall, M., Gao, Y., Ma, M., Lenger, K.A., Ko, E., Wright, K.D., Wang, H. and Stankovic, J., 2021. “Smarthealth technology study protocol to improve relationships between older adults with dementia and family caregivers”, Journal of Advanced Nursing, 77(5), pp.2519-2529.</li>
      </ul>

      <h5>2020</h5>
      <ul>
        <li> [SENSYS’20] Gao, Y., Ma, M., Gordon, K., Rose, K., Wang, H. and Stankovic, J., 2020, November. “A monitoring, modeling, and interactive recommendation system for in-home caregivers: Demo abstract”, In Proceedings of the 18th Conference on Embedded Networked Sensor Systems (pp. 587-588).</li>
      </ul>
    </div>

<!--
<ul>
	<li>Nikita Dhawan, Sicong Huang, Juhan Bae, and Roger Grosse. <a href="https://proceedings.mlr.press/v202/dhawan23a.html">Efficient Parametric Approximations of Neural Network Function Space Distance.</a>. ICML 2023.</li>
</ul>
<ul>
	<li>Juhan Bae, Michael R. Zhang, Michael Ruan, Eric Wang, So Hasegawa, Jimmy Ba, and Roger Grosse. <a href="https://arxiv.org/abs/2212.03905">Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve</a>. ICLR 2023.</li>
</ul>
<ul>
	<li>Stephen Zhao, Chris Lu, Roger Grosse, and Jakob Foerster. <a href="https://arxiv.org/abs/2210.10125">Proximal learning with opponent learning awareness</a>. NeurIPS 2022.</li>
</ul>
<ul>
	<li>Juhan Bae, Paul Vicol, Jeff Z. HaoChen, and Roger Grosse. <a href="https://arxiv.org/abs/2203.00089">Amortized proximal optimization</a>. NeurIPS 2022.</li>
</ul>
<ul>
	<li>Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse. <a href="https://arxiv.org/abs/2209.05364">If influence functions are the answer, then what is the question?</a>  NeurIPS 2022.</li>
</ul>
<ul>
	<li>Cem Anil, Ashwini Pokle, Kaiqu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie Bai, J. Zico Kolter, and Roger Grosse. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/331c41353b053683e17f7c88a797701d-Abstract-Conference.html">Path independent equilibrium networks can better exploit test-time computation</a>. NeurIPS 2022.</li>
</ul>
<ul>
	<li>Paul Vicol, Jonathan Lorraine, Fabian Pedregosa, David Duvenaud, and Roger Grosse. <a href="https://proceedings.mlr.press/v162/vicol22a.html">On implicit bias in overparameterized bilevel optimization</a>. ICML 2022.</li>
</ul>
<ul>
	<li>Rob Brekelmans, Sicong Huang, Marzyeh Ghassemi, Greg ver Steeg, Roger Grosse, and Alireza Makhzani. <a href="https://openreview.net/pdf?id=T0B9AoM_bFg">Improving Mutual Information Estimation with Annealed and Energy-Based Bounds</a>. ICLR 2022.</li>
</ul>
<ul>
	<li>Guodong Zhang, Kyle Hsu, Jianing Li, Chelsea Finn, and Roger Grosse. <a href="https://proceedings.neurips.cc/paper/2021/hash/a1a609f1ac109d0be28d8ae112db1bbb-Abstract.html">Differentiable Annealed Importance Sampling and the Perils of Gradient Noise</a>. NeurIPS 2021.</li>
</ul>
<ul>
	<li>Shengyang Sun, Jiaxin Shi, Andrew Gordon Wilson, and Roger Grosse. <a href="https://arxiv.org/abs/2106.05992">Scalable Variational Gaussian Processes via Harmonic Kernel Decomposition</a>. ICML 2021. <a href="https://github.com/ssydasheng/Harmonic-Kernel-Decomposition">[Code]</a></li>
</ul>
<ul>
	<li>James Lucas, Juhan Bae, Michael Zhang, Stanislav Fort, Richard Zemel, and Roger Grosse. <a href="https://arxiv.org/abs/2104.11044">Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes</a>. ICML 2021.</li>
</ul>
<ul>
	<li>Yuhuai Wu, Markus Rabe, Wenda Li, Jimmy Ba, Roger Grosse, and Christian Szegedy. <a href="https://arxiv.org/abs/2101.06223">LIME: Learning inductive bias for primitives of mathematical reasoning</a>. ICML 2021.</li>
</ul>
<ul>
	<li>Guodong Zhang, Xuchan Bao, Laurent Lessard, and Roger Grosse. <a href="https://jmlr.org/papers/v22/20-1068.html">A unified analysis of first-order methods for smooth games via integral quadratic constraints</a>. JMLR 2021. <a href="https://github.com/gd-zhang/IQC-Game">[Code]</a></li>
</ul>
<ul>
	<li>Chaoqi Wang, Shengyang Sun, and Roger Grosse. <a href="https://arxiv.org/abs/2011.03178">Beyond marginal uncertainty: How accurately can Bayesian regression models estimate posterior predictive correlations?</a> AISTATS 2021. <a href="https://github.com/ssydasheng/predictive-correlation-benchmark">[Code]</a></li>
</ul>
<ul>
	<li>Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and Jorn-Henrik Jacobsen. <a href="https://arxiv.org/abs/2006.09347">Understanding and mitigating exploding inverses in invertible neural networks</a>. AISTATS 2021. <a href="https://github.com/asteroidhouse/INN-exploding-inverses">[Code]</a></li>
</ul>
<ul>
	<li>Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Grosse. <a href="https://arxiv.org/abs/2007.02924">INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving</a>. ICLR 2021. <a href="https://github.com/albertqjiang/INT">[Code]</a></li>
</ul>
<ul>
	<li>Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuku, Denny Wu, and Ji Xu. <a href="https://arxiv.org/abs/2006.10732">When does preconditioning help or hurt generalization?</a> ICLR 2021.</li>
</ul>
<ul>
	<li>Pashootan Vaezipoor, Gil Lederman, Yuhuai Wu, Chris J. Maddison, Roger Grosse, Edward Lee, Sanjit A. Seshia, and Fahiem Bacchus. <a href="https://arxiv.org/abs/2007.03204">Learning Branching Heuristics for Propositional Model Counting</a>. AAAI 2021.</li>
</ul>
<ul>
	<li>Juhan Bae and Roger Grosse. <a href="https://proceedings.neurips.cc//paper/2020/hash/f754186469a933256d7d64095e963594-Abstract.html">Delta-STN: Efficient bilevel optimization of neural networks using structured response Jacobians</a>. NeurIPS 2020. <a href="https://github.com/pomonam/Self-Tuning-Networks">[Code]</a></li>
</ul>
<ul>
	<li>Xuchan Bao, James Lucas, Sushant Sachdeva, and Roger Grosse. <a href="https://arxiv.org/abs/2007.06731">Regularized linear autoencoders recover the principal components, eventually</a>. NeurIPS 2020. <a href="https://github.com/XuchanBao/linear-ae">[Code]</a></li>
</ul>
<ul>
	<li>Sheldon Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. <a href="https://arxiv.org/abs/2008.06653">Evaluating lossy compression rates of deep generative models</a>. ICML 2020. <a href="https://github.com/BorealisAI/rate_distortion">[Code]</a></li>
</ul>
<ul>
	<li>Chaoqi Wang, Guodong Zhang, and Roger Grosse. <a href="https://arxiv.org/abs/2002.07376">Picking winning tickets before training by preserving gradient flow</a>. ICLR 2020. <a href="https://github.com/alecwangcq/GraSP">[Code]</a></li>
</ul>
<ul>
	<li>Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, Christopher J. Shallue, and Roger Grosse. <a href="https://arxiv.org/abs/1907.04164">Which algorithmic choices matter at which batch sizes? Insights from a noisy quadratic model</a>. NeurIPS 2019. <a href="https://github.com/gd-zhang/noisy-quadratic-model">[Code]</a></li>
</ul>
<ul>
	<li>Guodong Zhang, James Martens, and Roger Grosse. <a href="https://arxiv.org/abs/1905.10961">Fast convergence of natural gradient descent for overparameterized neural networks</a>. NeurIPS 2019.</li>
</ul>
<ul>
	<li>James Lucas, George Tucker, Roger Grosse, and Mohammad Norouzi. <a href="http://papers.nips.cc/paper/9138-dont-blame-the-elbo-a-linear-vae-perspective-on-posterior-collapse.pdf">Don&#8217;t blame the ELBO!  A linear VAE perspective on posterior collapse</a>. NeurIPS 2019. <a href="https://colab.research.google.com/github/google-research/google-research/blob/master/linear_vae/DontBlameTheELBO.ipynb">[Code]</a></li>
</ul>
<ul>
	<li>Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger Grosse, and Jorn-Henrik Jacobsen. <a href="https://papers.nips.cc/paper/9673-preventing-gradient-attenuation-in-lipschitz-constrained-convolutional-networks.pdf">Preventing gradient attenuation in Lipschitz-constrained convolutional networks</a>. NeurIPS 2019. <a href="https://github.com/ColinQiyangLi/LConvNet">[Code]</a></li>
</ul>
<ul>
	<li>Cem Anil, James Lucas, and Roger Grosse. <a href="https://arxiv.org/abs/1811.05381">Sorting out Lipschitz function approximation</a>. ICML 2019. <a href="https://github.com/cemanil/LNets">[Code]</a></li>
</ul>
<ul>
	<li>Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. <a href="https://arxiv.org/abs/1905.05934">EigenDamage: structured pruning in the Kronecker-factored eigenbasis</a>. ICML 2019. <a href="https://github.com/alecwangcq/EigenDamage-Pytorch">[Code]</a></li>
</ul>
<ul>
	<li>Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger Grosse. <a href="https://arxiv.org/abs/1903.03088">Self-tuning networks: bilevel optimization of hyperparameters using structured best-response functions</a>. ICLR 2019. <a href="https://github.com/asteroidhouse/self-tuning-networks">[Code]</a></li>
</ul>
<ul>
	<li>Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. <a href="https://arxiv.org/abs/1903.05779">Functional variational Bayesian neural networks</a>. ICLR 2019. <a href="https://github.com/ssydasheng/FBNN">[Code]</a></li>
</ul>
<ul>
	<li>Sheldon Huang, Qiyang Li, Cem Anil, Xuchan Bao, Sageev Oore, and Roger Grosse. <a href="https://arxiv.org/abs/1811.09620">TimbreTron: A WaveNet ( CycleGAN ( CQT ( audio ))) pipeline for musical timbre transfer</a>. ICLR 2019.</li>
</ul>
<ul>
	<li>Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. <a href="https://arxiv.org/abs/1810.12281">Three mechanisms of weight decay regularization</a>. ICLR 2019. <a href="https://github.com/gd-zhang/Weight-Decay">[Code]</a></li>
</ul>
<ul>
	<li>James Lucas, Shengyang Sun, Richard Zemel, and Roger Grosse. <a href="https://arxiv.org/abs/1804.00325">Aggregated momentum: stability through passive damping</a>. ICLR 2019. <a href="https://github.com/AtheMathmo/AggMo">[Code]</a></li>
</ul>
<ul>
	<li>Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger Grosse. <a href="https://arxiv.org/abs/1810.10999">Reversible recurrent neural networks</a>. NIPS 2018. <a href="https://github.com/matthewjmackay/reversible-rnn">[Code]</a></li>
</ul>
<ul>
	<li>Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. <a href="https://arxiv.org/abs/1802.04942">Isolating sources of disentanglement in variational autoencoders</a>.  NIPS 2018. <a href="https://github.com/rtqichen/beta-tcvae">[Code]</a></li>
</ul>
<ul>
	<li>Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, and Roger Grosse. <a href="https://arxiv.org/abs/1806.04326">Differentiable compositional kernel learning for Gaussian processes</a>. ICML 2018. <a href="https://github.com/ssydasheng/Neural-Kernel-Network">[Code]</a></li>
</ul>
<ul>
	<li>Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. <a href="https://arxiv.org/abs/1712.02390">Noisy natural gradient as variational inference</a>. ICML 2018. [Code: <a href="https://github.com/gd-zhang/noisy-K-FAC">1</a>, <a href="https://github.com/pomonam/NoisyNaturalGradient">2</a>]</li>
</ul>
<ul>
	<li>Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger Grosse, and Richard Zemel. <a href="https://arxiv.org/abs/1806.10317">Adversarial distillation of Bayesian neural network posteriors</a>. ICML 2018. <a href="https://github.com/wangkua1/apd_public">[Code]</a></li>
</ul>
<ul>
	<li>Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. <a href="https://arxiv.org/abs/1803.04386">Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches</a>. ICLR 2018.</li>
</ul>
<ul>
	<li>Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. <a href="https://arxiv.org/abs/1803.02021">Understanding short-horizon bias in stochastic meta-optimization</a>. ICLR 2018.</li>
</ul>
<ul>
	<li>Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, and Jimmy Ba. <a href="https://arxiv.org/abs/1708.05144">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a>. NIPS 2017.
	<ul>
		<li><a href="https://github.com/openai/baselines" title="OpenAI Baselines">code</a></li>
	</ul></li>
</ul>
<ul>
	<li>Aidan Gomez, Mengye Ren, Raquel Urtasun, and Roger Grosse. <a href="https://arxiv.org/abs/1707.04585">The Reversible Residual Network: Backpropagation Without Storing Activations</a>. NIPS 2017.
	<ul>
		<li><a href="https://github.com/renmengye/revnet-public">code</a></li>
	</ul></li>
</ul>
<ul>
	<li>Jacob Gardner, Chuan Guo, Kilian Weinberger, Roman Garnett, and Roger Grosse. <a href="aistats2017-additive.pdf">Discovering and exploiting additive structure for Bayesian optimization</a>. AISTATS 2017.</li>
</ul>
<ul>
	<li>Jimmy Ba, Roger Grosse, and James Martens. <a href="https://jimmylba.github.io/papers/nsync.pdf">Distributed second-order optimization using Kronecker-factored approximations</a>. ICLR 2017.</li>
</ul>
<ul>
	<li>Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. <a href="http://openreview.net/pdf?id=B1M8JF9xx">On the quantitative analysis of decoder-based generative models.</a>. ICLR 2017.
	<ul>
		<li><a href="https://github.com/tonywu95/eval_gen">code</a></li>
	</ul></li>
</ul>
<ul>
	<li>Roger Grosse, Siddharth Ancha, and Daniel Roy. <a href="nips2016-bread.pdf">Measuring the reliability of MCMC inference with bidirectional Monte Carlo</a>. NIPS 2016.
	<ul>
		<li><a href="http://arxiv.org/abs/1606.02275">arXiv</a></li>
	</ul></li>
</ul>
<ul>
	<li>Roger Grosse and James Martens. <a href="http://arxiv.org/abs/1602.01407">A Kronecker-factored approximate Fisher matrix for convolution layers</a>. ICML 2016.
	<ul>
		<li><a href="icml2016-kfc.pdf">ICML version</a></li>
	</ul></li>
</ul>
<ul>
	<li>Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. <a href="http://arxiv.org/abs/1509.00519">Importance weighted autoencoders</a>. ICLR 2016.
	<ul>
		<li><a href="https://github.com/yburda/iwae">code</a></li>
	</ul></li>
</ul>
<ul>
	<li>Jimmy Ba, Roger Grosse, Ruslan Salakhutdinov, and Brendan Frey. <a href="http://arxiv.org/abs/1509.06812">Learning wake-sleep recurrent attention models</a>. NIPS 2015.
	<ul>
		<li><a href="https://papers.nips.cc/paper/5861-learning-wake-sleep-recurrent-attention-models">NIPS version</a></li>
	</ul></li>
</ul>
<ul>
	<li>Roger Grosse and Ruslan Salakhutdinov. <a href="icml2015-fang.pdf">Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix</a>. ICML 2015.
	<ul>
		<li><a href="https://github.com/rgrosse/fang">code</a></li>
	</ul></li>
</ul>
<ul>
	<li>James Martens and Roger Grosse. <a href="http://arxiv.org/abs/1503.05671">Optimizing Neural Networks with Kronecker-factored Approximate Curvature</a>. ICML 2015.
	<ul>
		<li><a href="icml2015-kfac.pdf">ICML version</a>, and <a href="icml2015-kfac-appendix.pdf">appendix</a> (terser and less readable than the arXiv version)</li>
	</ul></li>
</ul>
<ul>
	<li>Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. <a href="http://arxiv.org/abs/1412.8566">Accurate and conservative estimates of MRF log-likelihood using reverse annealing</a>. AISTATS 2015.</li>
</ul>
<ul>
	<li>James R. Lloyd, David Duvenaud, Roger B. Grosse, Joshua B. Tenenbaum, and Zoubin Ghahramani. <a href="aaai2014-autostat.pdf">Automatic construction and natural-language description of nonparametric regression models</a>. AAAI 2014.
	<ul>
		<li><a href="http://github.com/jamesrobertlloyd/gpss-research">code</a></li>
		<li><a href="http://mlg.eng.cam.ac.uk/Lloyd/abcdoutput/">examples</a></li>
	</ul></li>
</ul>
<ul>
	<li>Roger B. Grosse, Chris J. Maddison, and Ruslan Salakhutdinov. <a href="nips2013-moment.pdf">Annealing between distributions by averaging moments</a>. NIPS 2013.
	<ul>
		<li><a href="nips2013-moment-supplemental.pdf">supplemental material</a></li>
		<li><a href="icml2013-moment.pdf">preprint</a> (from the ICML 2013 workshop <a href="http://deeplearning.net/icml2013-workshop-competition/">Challenges in Representation Learning</a>)</li>
		<li><a href="http://www.metacademy.org/roadmaps/rgrosse/nips2013">background</a></li>
	</ul></li>
</ul>
<ul>
	<li>David Duvenaud, James R. Lloyd, Roger B. Grosse, Joshua B. Tenenbaum, and Zoubin Ghahramani. <a href="icml2013-gp.pdf">Structure discovery in nonparametric regression through compositional kernel search</a>. ICML 2013.
	<ul>
		<li><a href="https://github.com/jamesrobertlloyd/gp-structure-search">code</a></li>
		<li><a href="http://www.metacademy.org/roadmaps/rgrosse/icml2013">background</a></li>
	</ul></li>
</ul>
<ul>
	<li>Roger B. Grosse, Ruslan Salakhutdinov, William T. Freeman, and Joshua B. Tenenbaum. <a href="uai2012-matrix.pdf">Exploiting compositionality to explore a large space of model structures</a>. UAI 2012. <strong>Best Student Paper.</strong>
	<ul>
		<li><a href="https://github.com/rgrosse/compositional_structure_search">code</a></li>
		<li><a href="http://www.metacademy.org/roadmaps/rgrosse/uai2012">background</a></li>
	</ul></li>
</ul>
<ul>
	<li>Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. <a href="cacm2011-cdbn.pdf">Unsupervised learning of hierarchical representations with convolutional deep belief networks</a>. <em>Communications of the ACM</em>, vol. 54, no. 10, pp. 95-103, 2011.</li>
</ul>
<ul>
	<li>Roger Grosse, Micah K. Johnson, Edward Adelson, and William T. Freeman. <a href="iccv09-intrinsic.pdf">A ground-truth dataset and baseline evaluations for intrinsic image algorithms</a>. ICCV 2009.
	<ul>
		<li><a href="http://www.cs.toronto.edu/~rgrosse/intrinsic">project page</a></li>
	</ul></li>
</ul>
<ul>
	<li>Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. <a href="icml09-cdbn.pdf">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</a>. ICML 2009. <strong>Best Application Paper</strong></li>
</ul>
<ul>
	<li>Roger Grosse, Rajat Raina, Helen Kwong, and Andrew Y. Ng. <a href="uai07-sisc.pdf">Shift-invariant sparse coding for audio classification</a>. UAI 2007
	<ul>
		<li><a href="uai07-sisc-code.tar.gz">code</a></li>
	</ul></li>
</ul>
<h3>Preprints</h3>
<ul>
	<li>Cem Anil, Guodong Zhang, Yuhuai Wu, and Roger Grosse, 2021. <a href="https://arxiv.org/abs/2108.12099">Learning to Give Checkable Answers with Prover-Verifier Games</a>.</li>
</ul>
<ul>
	<li>Yuhuai Wu, Honghua Dong, Roger Grosse, and Jimmy Ba, 2020. <a href="https://arxiv.org/abs/2007.04212">The Scattering Compositional Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning</a>.</li>
</ul>
<ul>
	<li>Kevin Luk and Roger Grosse, 2018. <a href="https://arxiv.org/abs/1808.10340">A coordinate-free construction of scalable natural gradient</a>.</li>
</ul>
<ul>
	<li>Roger Grosse, Zoubin Ghahramani, and Ryan Adams, 2015. <a href="http://arxiv.org/abs/1511.02543">Sandwiching the marginal likelihood using bidirectional Monte Carlo</a>.</li>
</ul>
<h3>Thesis</h3>
<ul>
	<li><a href="phd-thesis.pdf">Model selection in compositional spaces</a>. Ph.D. thesis, 2014.</li>
</ul>
   </div>

-->


   <div class="column span-15 prepend-2 append-2 first last" id="footer">
     <hr />
   </div>

</div>
</body>
</html>
